{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for starting this project\n",
    "\n",
    "## Our main goals were to:\n",
    "\n",
    "1. Take a repetitive task done in Excel and attempt to rewrite it using Python;\n",
    "2. Along the way we hoped to get better understanding of the questions, methodologies and ways of working of Research;\n",
    "3. Learn how to use Python efficiently on a day-to-day basis.\n",
    "\n",
    "## Within the wider Research environment framework, we want to begin to answer questions such as:\n",
    "1. How to write scripts to automate the downloading of data.\n",
    "2. Store downloaded data in an organized, durable, manner.\n",
    "3. Store intermediate results.\n",
    "4. Assess the quality of data.\n",
    "\n",
    "## The specfic question addressed here:\n",
    "\n",
    "In particular, Travis wanted a means to calculate and compare correlations of commodity time series.  The series are currently stored in Excel sheets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell in most scripts import/load the necessary Python packages and sets global parameters.   The packages we used in this project are very commonly used throughout the Python community.  Pandas in particular is a very convenient container that looks a lot like Excel and other spreadsheets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import xlrd  #needed to read \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/picOrgData1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the most complex part of the script.  It is a def() or function that Python uses to repeat competitive tasks, you can think of it as a very flexible macro.\n",
    "\n",
    "Function which takes a Excel (.xlsx) file and converts it to a Pandas dataframe.\n",
    "It loops through each specified sheet of the Excel file and puts the resulting Pandas dataframe into a list of dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromXLSXtoDF(sheetNames_param, xlsxData_param):\n",
    "    allDF = []\n",
    "    for i in sheetNames_param:\n",
    "        # print the sheet name (not necessary) but print statements give valuable feedback to the programmer and are used everywhere \n",
    "        # in the development process.\n",
    "        print(\"Reading in sheet:\", i)\n",
    "        # so read each sheet named \"i\", and grab only columns 0 and 5 (which contain the relevant data)\n",
    "        df = pd.read_excel(xlsxData_param, sheet_name=i, usecols=[0, 5])\n",
    "        # set the timestamp to the index, this makes time series analysis easier.\n",
    "        df = df.set_index(\"Date\")\n",
    "        # calculate diff and percent change, should be done per year\n",
    "        # diff is the first difference, so the change in data from one day to the next.  \n",
    "        df[\"PremiumDiff\"] = df[\"Premium\"].diff(1)\n",
    "        # percent change is the change from one day to the next.\n",
    "        df[\"PremiumPrctChange\"] = df[\"Premium\"].pct_change(1)\n",
    "        # add the dataframe to the list of dataframes\n",
    "        allDF.append(df)\n",
    "    # concatenate across column (axis=0), use the index as key)\n",
    "    result = pd.concat(allDF, axis=0, join='outer')\n",
    "    # change names of columns to the sheetnames\n",
    "    # result.columns = sheetNames\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes the required names of the Excel sheets and the Excel file and runs the function fromXLSXtoDF defined in the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in sheet: SB Par Mar17\n",
      "Reading in sheet: SB Par Mar18\n",
      "Reading in sheet: SB Par Mar19\n"
     ]
    }
   ],
   "source": [
    "#Calculate correlations for raw data\n",
    "\n",
    "#Sheet that contain the desired data\n",
    "sheetNames = [\"SB Par Mar17\", \"SB Par Mar18\", \"SB Par Mar19\"]\n",
    "\n",
    "#Read in the name of the Excel file\n",
    "xlsxData = 'Pna Prem vs Brl - Jeff.xlsx'\n",
    "\n",
    "#Run the function\n",
    "sbYears = fromXLSXtoDF(sheetNames, xlsxData)\n",
    "\n",
    "#Drop missing values\n",
    "sbYears = sbYears.dropna()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads the Brazilian currency data.  We didn't write a function because we only do these steps once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in BRL dta\n",
    "\n",
    "#Read in data from the same Excel file in sheet named 'Brl'\n",
    "brl = pd.read_excel('Pna Prem vs Brl - Jeff.xlsx', 'Brl')\n",
    "\n",
    "#Set Date (GMT) as the index, which makes times series analysis easier\n",
    "brl = brl.set_index('Date (GMT)')\n",
    "\n",
    "#Re-naming index to Date in order to merge \n",
    "brl.index.name = 'Date' \n",
    "\n",
    "#Calculate the percentage change, just like in the function \n",
    "brl['BrlPrctChange'] = brl[\"Last\"].pct_change(1)\n",
    "\n",
    "#Calculate the first differeence, just like in the function\n",
    "brl['BrlDiff'] = brl[\"Last\"].diff(1)\n",
    "\n",
    "#Take only the relevant columns of the Pandas dataframe\n",
    "brl = brl[[\"Last\", \"BrlDiff\", \"BrlPrctChange\"]]\n",
    "\n",
    "#Rename the columns called Last to Brl\n",
    "brl = brl.rename(columns = {'Last':'Brl'})\n",
    "\n",
    "#Drop missing values\n",
    "brl = brl.dropna()\n",
    "\n",
    "#Show me the results\n",
    "brl.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two data frames created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "merD = pd.merge(sbYears,brl,how='inner',on='Date')\n",
    "merD.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not continuous over the years, so only take those days for which data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = merD\n",
    "year2016 = df1['2016-06-07': '2016-09-29']\n",
    "print(\"2016:\", year2016.shape)  #print states to find out how much data each dataframe holds\n",
    "year2017 = df1['2017-06-05': '2017-09-29']\n",
    "print(\"2017:\", year2017.shape)\n",
    "year2018 = df1['2018-06-05': '2018-09-29']\n",
    "print(\"2018:\", year2018.shape)\n",
    "year2019 = df1['2019-06-05': '2019-09-29']\n",
    "print(\"2019:\", year2019.shape)  #2019 is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data.  The difference in scale of the data distorts any visual representation in the realationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year2017[['PremiumPrctChange', 'BrlPrctChange']].plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the data based on the minimum and maximum for each column of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize data, makes comparisons easier\n",
    "def scaleDF(myDF):\n",
    "    resultsStd = myDF.values #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    results_scaled = min_max_scaler.fit_transform(resultsStd)\n",
    "    myresults_scaled = pd.DataFrame(results_scaled)\n",
    "    \n",
    "    # change names of columns to the sheetnames\n",
    "    myresults_scaled.columns = myDF.columns\n",
    "    \n",
    "    # add index back in\n",
    "    indx = myDF.index\n",
    "    myresults_scaled.set_index(indx, inplace=True)\n",
    "    \n",
    "    return(myresults_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfScale = scaleDF(merD)\n",
    "dfScale.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replot the scaled data.  Patterns in the data are more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year2017 = dfScale['2017-06-05': '2017-09-29']\n",
    "year2017[['PremiumPrctChange', 'BrlPrctChange']].plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add year, month and day columns to the data for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfScale  #Rename scaled data\n",
    "\n",
    "df1[\"year\"] = df1.index.year\n",
    "df1[\"month\"] = df1.index.month\n",
    "df1[\"day\"] = df1.index.day\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same trick of subsetting the months for which we have data.\n",
    "year2016 = df1['2016-06-07': '2017-03-15']\n",
    "year2017 = df1['2017-06-05': '2018-03-15']\n",
    "year2018 = df1['2018-06-05': '2019-03-15']\n",
    "year2019 = df1['2019-06-05': '2020-03-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot SPX and VIX cumulative returns with recession overlay\n",
    "plot_cols = ['Premium', 'Brl']\n",
    "fig, axes = plt.subplots(2,1, figsize=(15,7), sharex=True, sharey=True)\n",
    "year2016[plot_cols].plot(subplots=True, ax=axes)\n",
    "fig, axes = plt.subplots(2,1, figsize=(15,7), sharex=True, sharey=True)\n",
    "year2017[plot_cols].plot(subplots=True, ax=axes)\n",
    "fig, axes = plt.subplots(2,1, figsize=(15,7), sharex=True, sharey=True)\n",
    "year2018[plot_cols].plot(subplots=True, ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "ARIMA\n",
    "Granger causality\n",
    "Cointegration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA models can be used to predict future series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Premium\"]['2018-06-05': '2019-03-15'].plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    " \n",
    "series = df1[\"Premium\"]['2018-06-05': '2019-03-15']\n",
    "# fit model\n",
    "model = ARIMA(series, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "# plot residual errors\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations per month, 20 day  June 1st to June 20st = corr; June 2nd to June 21st..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled data frame\n",
    "dfScale = scaleDF(merD)\n",
    "dfScale.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a var model to examine how the two time series affect one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statsmodels\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future projects: backtesting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
